# HardNet: Hard-Constrained Neural Networks with Universal Approximation Guarantees 

Youngjae Min<br>YJM@MIT.EDU<br>Massachusetts Institute of Technology

Navid Azizan<br>AZIZAN@MIT.EDU<br>Massachusetts Institute of Technology


#### Abstract

Incorporating prior knowledge or specifications of input-output relationships into machine learning models has attracted significant attention, as it enhances generalization from limited data and yields conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction, especially on inputs far from the training distribution-an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method that enables efficient and differentiable enforcement of more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closedform enforcement layer to the network's output. Furthermore, we show that HardNet retains neural networks' universal approximation capabilities. We demonstrate its versatility and effectiveness across various applications: learning with piecewise constraints, learning optimization solvers with guaranteed feasibility, and optimizing control policies in safetycritical systems. ${ }^{1}$


Keywords: constrained neural networks, physics-constrained machine learning, safetycritical systems, control theory, optimization

## 1 Introduction

Neural networks are widely adopted for their generalization capabilities and their ability to model highly nonlinear functions in high-dimensional spaces. With their increasing proliferation, it has become more important to be able to impose constraints on neural networks in many applications. By incorporating domain knowledge about input-output relationships through constraints, we can enhance generalization, particularly when available data is limited (Pathak et al., 2015; Oktay et al., 2017; Raissi et al., 2019). These constraints introduce inductive biases that guide the model's learning process toward plausible solutions that adhere to known properties of the problem domain, potentially reducing overfitting. Consequently, neural networks can more effectively capture underlying patterns and make accurate predictions on unseen data, despite scarce training samples.

[^0]Moreover, adherence to specific requirements is critical in many practical applications. For instance, in robotics, this could translate to imposing collision avoidance or ensuring configurations remain within a valid motion manifold (Ding and Fan, 2014; Wang and Yan, 2023; Ryu et al., 2022; Huang et al., 2022). In geometric learning, this could mean imposing a manifold constraint (Lin and Zha, 2008; Simeonov et al., 2022). In financial risk management, violating constraints on the solvency of the portfolio can lead to large fines (McNeil et al., 2015). Enforcing neural network outputs to satisfy these non-negotiable rules (i.e., hard constraints) makes models more reliable, interpretable, and aligned with the underlying problem structure.

However, introducing hard constraints can potentially limit a neural network's expressive power. To illustrate this point, consider a constraint that requires the model's output to be less than 1 . One could simply restrict the model to always output a constant value less than 1 , which ensures the constraint satisfaction but obviously limits the model capacity drastically. This raises the question:
Can we enforce hard constraints on neural networks without losing their expressive power?
The model capacity of neural networks is often explained through the universal approximation theorem, which shows that a neural network can approximate any continuous function given a sufficiently wide/deep architecture. Demonstrating that this theorem still holds under hard constraints is essential to understanding the trade-off between constraint satisfaction and model capacity.

Contributions We tackle the problem of enforcing hard constraints on neural networks, namely:

- We present a practical framework called HardNet (short for hard-constrained neural network) for constructing neural networks that satisfy input-dependent constraints by construction. HardNet is, to the best of our knowledge, the first method that enables efficient and differentiable enforcement of more than one input-dependent inequality constraint. It allows for unconstrained optimization of the networks' parameters with standard algorithms.
- We prove universal approximation theorems for our method, showing that despite enforcing the hard constraints, our construction retains the expressive power of neural networks, i.e., it provably does not overconstrain the model.
- We demonstrate the utility of our method on a variety of scenarios where it is critical to satisfy hard constraints-learning with piecewise constraints, learning optimization solvers with guaranteed feasibility, and optimizing control policies in safety-critical systems.
- We provide the first systematic taxonomy and comparative analysis of hardconstrained neural networks (Table 1)-aligning constraint type, input-dependence, guarantees, cost, and expressivity.


## 2 Related Work

Neural Networks with Soft Constraints Early approaches used data augmentation or domain randomization to structure the dataset to satisfy the necessary constraints before training the neural network. Other initial directions focused on introducing the constraints

Table 1: Comparison of methods enforcing hard constraints on neural networks for the target function $y=f(x) \in \mathbb{R}^{n_{\text {out }}}$. HardNet-Aff is the only method to enforce input-dependent constraints provably and efficiently with universal approximation guarantees. (Eq.: Equality, Ineq.: Inequality)
| Method | Constraint | Input Depend. | Support Eq./Ineq. | Satisfaction Guarantee | Computation | Universal Approx. |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Soft-Constrained | Any | Yes | Both | No | Closed-Form | Yes |
| Frerix et al. (2020, | $A y \leq 0$ | No | Both | Always | Closed-Form | Unknown |
| LinSATNet (Wang et al., 2023) | $A_{1} y \leq b_{1}, A_{2} y \geq b_{2} \left(y \in[0,1]^{n_{\text {out }}}, A_{*}, b_{*} \geq 0\right)$ | No | Both | Asymptotic | Iterative | Unknown |
| C-DGM (Stoian et al., 2024 | $A y \leq b$ | No | Both | Always | Closed-Form | Unknown |
| RAYEN (Tordesillas et al., 2023 | $y \in \mathcal{C}(\mathcal{C}$ : linear, quadratic, SOC, LMI) | No | Both | Always | Closed-Form | Unknown |
| POLICE (Balestriero and LeCun, 2023) | $y=A x+b \forall x \in R$ | Yes | Eq. Only | Always | Closed-Form | Unknown |
| KKT-hPINN (Chen et al., 2024) | $\begin{gathered} A x+B y=b \\ \left(\# \text { constraints } \leq n_{\text {out }}\right) \end{gathered}$ | Yes | Eq. Only | Always | Closed-Form | Unknown |
| ACnet (Beucler et al., 2021) | $\begin{gathered} h_{x}(y)=0 \\ \left(\# \text { constraints } \leq n_{\text {out }}\right) \end{gathered}$ | Yes | Eq. Only | Always | Closed-Form | Unknown |
| DC3 (Donti et al., 2021b) | $g_{x}(y) \leq 0, h_{x}(y)=0$ | Yes | Both | Asymptotic for linear $g_{x}, h_{x}$ | Iterative | Unknown |
| HardNet-Aff (Ours) | $b^{l}(x) \leq A(x) y \leq b^{u}(x)$ <br> (\# constraints $\leq 2 n_{\text {out }}$ ) | Yes | Both | Always | Closed-Form | Yes |


as soft penalties (MÃ¡rquez-Neila et al., 2017; Dener et al., 2020) into the cost function of the neural network along with penalty weights or Lagrange multipliers as hyperparameters. Raissi et al. (2019); Li et al. (2024) leveraged this idea in their work on physics-informed neural networks (PINNs) to enforce that the output satisfies a given differential equation. In parallel to these penalty-based and augmented Lagrangian methods, Chamon and Ribeiro (2020) Chamon et al. (2023) proposed rigorous methods that optimize over both primal and dual variables with distributional guarantees based on the PAC-learning framework. Hounie et al. (2023) extended this approach by adaptively relaxing constraints to find a better compromise between the objective and the constraints. While soft-constraint methods are useful for incentivizing the desirable behavior in the model, their main limitation is that they do not ensure constraint satisfaction for arbitrary inputs, especially those far from the training distribution.

Neural Networks with Hard Constraints Some conventional neural network components can already enforce specific types of hard constraints. For instance, sigmoids can impose lower and upper bounds, softmax layers enforce simplex constraints, and ReLU layers are projections onto the positive orthant. The convolution layer in ConvNets encodes a translational equivariance constraint, which led to significant improvements in empirical performance. Learning new equivariances and inductive biases that accelerate learning for specific tasks is an active research area.

Recent work has explored new architectures to (asymptotically) impose various hard constraints by either finding certain parameterizations of feasible sets or incorporating
differentiable projections into neural networks, as summarized in Table 1. Frerix et al. (2020) addressed homogeneous linear inequality constraints by embedding a parameterization of the feasible set in a neural network layer. Huang et al. (2021) and LinSATNet (Wang et al., 2023) introduced differentiable projection methods that iteratively refine outputs to satisfy linear constraints. However, these iterative approaches do not guarantee constraint satisfaction within a fixed number of iterations, limiting their reliability in practice. C-DGM (Stoian et al., 2024) enforces linear inequality constraints in generative models for tabular data by incrementally adjusting each output component in a finite number of iterations. However, its application to input-dependent constraints is limited as it cannot efficiently handle batched data. When constraints are input-dependent, the method requires recomputing the reduced constraint sets for each input, making it computationally prohibitive. In the context of optimal power flow, Chen et al. (2023) enforces feasibility for learning optimization proxies through closed-form differentiable repair layers. While effective, this approach is restricted to specific affine constraints.

Beyond affine constraints, RAYEN (Tordesillas et al., 2023) and Konstantinov and Utkin (2023) enforce certain convex constraints by parameterizing the feasible set such that the neural network output represents a translation from an interior point of the convex feasible region. However, these methods are limited to constraints dependent only on the output, and not the input. Extending these methods to input-dependent constraints is challenging because it requires finding different parameterizations for each input, such as determining a new interior point for every feasible set.

Another line of work considers hard constraints that depend on both input and output. POLICE (Balestriero and LeCun, 2023) enforces the output to be an affine function of the input in specific regions by reformulating the neural networks as continuous piecewise affine mappings. KKT-hPINN (Chen et al., 2024) handles more general affine equality constraints by projecting the output to the feasible set where the projection is computed using KKT conditions. ACnet (Beucler et al., 2021) enforces nonlinear equality constraints by transforming them into affine constraints for redefined inputs and outputs. However, these methods are restricted to equality constraints. DC3 (Donti et al., 2021b) tackles more general nonlinear constraints by reducing inequality constraints violations via gradient-based methods over the manifold where equality constraints are satisfied. However, it does not guarantee constraint satisfaction in general and is sensitive to the number of gradient steps and the step size, which require fine-tuning.

More closely related to our framework, methods to enforce a single affine inequality constraint have been proposed in the control literature: Kolter and Manek (2019) presented a framework for learning a stable dynamical model that satisfies a Lyapunov stability constraint. Based on this method, Min et al. (2023) presented the CoILS framework to learn a stabilizing control policy for an unknown control system by enforcing a control Lyapunov stability constraint. Our work generalizes the ideas used in these works to impose more general affine/convex constraints while proving universal approximation guarantees that are absent in prior works; On the theoretical front, Kratsios et al. (2021) presented a constrained universal approximation theorem for probabilistic transformers whose outputs are constrained to be in a feasible set. However, their contribution is primarily theoretical, and they do not present a method for learning such a probabilistic transformer.

## HardNet: Hard-Constrained Neural Networks

Formal Verification of Neural Networks Verifying whether a provided neural network (after training) always satisfies a set of constraints for a certain set of inputs is a well-studied subject. Albarghouthi et al. (2021) provide a comprehensive summary of the constraintbased and abstraction-based approaches to verification. Constraint-based verifiers are often both sound and complete but they have not scaled to practical neural networks, whereas abstraction-based techniques are approximate verifiers which are sound but often incomplete (Bunel et al., 2018; Elboher et al., 2020; Brown et al., 2022; Tjeng et al., 2019; Liu et al., 2021; Fazlyab et al., 2020; Qin et al., 2019; Ehlers, 2017). Other approaches have focused on formally verified exploration and policy learning for reinforcement learning (Bastani et al., 2018; Anderson et al., 2020; Wabersich et al., 2022). Contrary to most formal verification methods, which take a pre-trained network and verify that its output always satisfies the desired constraints, our method guarantees constraint satisfaction by construction throughout the training.

To address constraint violations in trained models after verification, some safe reinforcement learning methods use shielding mechanisms by overriding unsafe decisions using a backup policy based on reachability analysis (Shao et al., 2021; Bastani et al., 2021). While effective, this setup introduces a hard separation between learning and enforcement, often sacrificing performance and limiting joint optimization. Shielding is typically not differentiable and cannot be integrated into training. Similarly, editing methods modify trained networks post hoc to enforce output constraints by solving relaxed optimization problems over the model parameters (Sotoudeh and Thakur, 2021; Tao and Thakur, 2024). Though recent works demonstrate their application during training, these techniques generally target input-independent constraints and are architecture-dependent.

Neuro-Symbolic AI HardNet also aligns with the objectives of Neuro-symbolic AI, a field that has gained significant attention in recent years for its ability to integrate complex background knowledge into deep learning models. Unlike HardNet, which focuses on algebraic constraints, the neuro-symbolic AI literature primarily addresses logical constraints. A common approach in this field is to softly impose constraints during training by introducing penalty terms into the loss function to discourage constraint violations (Xu et al., 2018; Fischer et al., 2019; Badreddine et al., 2022; Stoian et al., 2023). While these methods are straightforward to implement, they do not guarantee constraint satisfaction. In contrast, works such as Giunchiglia and Lukasiewicz (2020); Ahmed et al. (2022); Giunchiglia et al. (2024) ensure constraints are satisfied by embedding them into the predictive layer, thus guaranteeing compliance by construction. Another line of research maps neural network outputs into logical predicates, ensuring constraint satisfaction through reasoning on these predicates (Manhaeve et al., 2018; Pryor et al., 2023; van Krieken et al., 2023).

## 3 Preliminaries

### 3.1 Notation

For $p \in[1, \infty),\|v\|_{p}$ denotes the $\ell^{p}$-norm for a vector $v \in \mathbb{R}^{m}$, and $\|A\|_{p}$ denotes the operator norm for a matrix $A \in \mathbb{R}^{k \times m}$ induced by the $\ell^{p}$-norm, i.e., $\|A\|_{p}=\sup _{w \neq 0}\|A w\|_{p} /\|w\|_{p}$. $v_{(i)} \in \mathbb{R}$ denotes the $i$-th component of $v$. $[A ; B]$ denotes the row-wise concatenation of the matrices $A$ and $B$.

For a domain $\mathcal{X} \subset \mathbb{R}^{n_{\text {in }}}$ and a codomain $\mathcal{Y} \subset \mathbb{R}^{n_{\text {out }}}$, let $\mathcal{C}(\mathcal{X}, \mathcal{Y})$ be the class of continuous functions from $\mathcal{X}$ to $\mathcal{Y}$ endowed with the sup-norm: $\|f\|_{\infty}:=\sup _{x \in \mathcal{X}}\|f(x)\|_{\infty}$. Similarly, $L^{p}(\mathcal{X}, \mathcal{Y})$ denotes the class of $L^{p}$ functions from $\mathcal{X}$ to $\mathcal{Y}$ with the $L^{p}$-norm: $\|f\|_{p}:= \left(\int_{\mathcal{X}}\|f(x)\|_{p}^{p} d x\right)^{\frac{1}{p}}$. For function classes $\mathcal{F}_{1}, \mathcal{F}_{2} \subset \mathcal{C}(\mathcal{X}, \mathcal{Y})$ (respectively, $\mathcal{F}_{1}, \mathcal{F}_{2} \subset L^{p}(\mathcal{X}, \mathcal{Y})$ ), we say $\mathcal{F}_{1}$ universally approximates (or is dense in) $\mathcal{F}_{2}$ if for any $f_{2} \in \mathcal{F}_{2}$ and $\epsilon>0$, there exists $f_{1} \in \mathcal{F}_{1}$ such that $\left\|f_{2}-f_{1}\right\|_{\infty} \leq \epsilon$ (respectively, $\left\|f_{2}-f_{1}\right\|_{p} \leq \epsilon$ ). For a neural network, its depth and width are defined as the total number of layers and the maximum number of neurons in any single layer, respectively.

### 3.2 Universal Approximation Theorem

The universal approximation property is a foundational concept in understanding the capabilities of neural networks in various applications. Classical results reveal that shallow neural networks with arbitrary width can approximate any continuous function defined on a compact set as formalized in the following theorem (Cybenko, 1989; Hornik et al., 1989; Leshno et al., 1993; Pinkus, 1999):

Theorem 1 (Universal Approximation Theorem for Shallow Networks) Let $\rho \in \mathcal{C}(\mathbb{R}, \mathbb{R})$ and $\mathcal{K} \in \mathbb{R}$ be a compact set. Then, depth-two neural networks with $\rho$ activation function universally approximate $\mathcal{C}(\mathcal{K}, \mathbb{R})$ if and only if $\rho$ is nonpolynomial.

To further understand the success of deep learning, the universal approximation property for deep and narrow neural networks has also been studied in the literature (Lu et al., 2017; Hanin and Sellke, 2017; Kidger and Lyons, 2020; Park et al., 2021). Interesting results show that a critical threshold exists on the width of deep networks that attain the universal approximation property. For instance, deep networks with ReLU activation function with a certain minimum width can approximate any $L^{p}$ function as described in the following theorem (Park et al., 2021, Thm. 1):

Theorem 2 (Universal Approximation Theorem for Deep Networks) For any $p \in [1, \infty)$, $w$-width neural networks with ReLU activation function universally approximate $L^{p}\left(\mathbb{R}^{n_{\text {in }}}, \mathbb{R}^{n_{\text {out }}}\right)$ if and only if $w \geq \max \left\{n_{\text {in }}+1, n_{\text {out }}\right\}$.

Despite these powerful approximation guarantees, they fall short when neural networks are required to satisfy hard constraints, such as physical laws or safety requirements. These theorems ensure that a neural network can approximate a target function arbitrarily closely but do not guarantee adherence to necessary constraints. Consequently, even if the target function satisfies specific hard constraints, the neural network approximator might violate them-especially in regions where the target function barely meets the constraints. This shortcoming is particularly problematic for applications that demand strict compliance with non-negotiable domain-specific rules. Therefore, ensuring that neural networks can both approximate target functions accurately and rigorously satisfy hard constraints remains a critical challenge for their deployment in practical applications.

## 4 HardNet: Hard-Constrained Neural Network

In this section, we present a practical framework, HardNet, shown in Figure 1, for enforcing input-dependent hard constraints on neural networks while retaining their universal

## HardNet: Hard-Constrained Neural Networks

![](https://cdn.mathpix.com/cropped/f547083e-7be0-42e8-9246-1bc5914287ea-07.jpg?height=338&width=1142&top_left_y=309&top_left_x=489)
Figure 1: Schematic of HardNet. Its differentiable enforcement layer allows unconstrained end-to-end optimization of the network parameters using standard algorithms while guaranteeing satisfaction with input-dependent constraints by construction. The layer can be applied to any neural networks.

approximation properties. In a nutshell, for a parameterized (neural network) function $f_{\theta}: \mathcal{X} \subset \mathbb{R}^{n_{\text {in }}} \rightarrow \mathbb{R}^{n_{\text {out }}}$, we ensure the satisfaction of given constraints by appending a differentiable enforcement layer with a projection $\mathcal{P}$ to $f_{\theta}$. This results in the projected function $\mathcal{P}\left(f_{\theta}\right): \mathcal{X} \rightarrow \mathbb{R}^{n_{\text {out }}}$ meeting the required constraints while allowing its output to be backpropagated through to train the model via gradient-based algorithms. Importantly, we show that the proposed architecture has universal approximation guarantees, i.e., it universally approximates the class of functions that satisfy the constraints.

A key challenge in this approach is devising a differentiable projection that has efficient forward and backward passes. For instance, consider affine constraints $A(x) f(x) \leq b(x) \forall x \in \mathcal{X}$ for a function $f: \mathcal{X} \rightarrow \mathbb{R}^{n_{\text {out }}}$ with $A(x) \in \mathbb{R}^{n_{\mathrm{c}} \times n_{\text {out }}}$ and $b(x) \in \mathbb{R}^{n_{\mathrm{c}}}$, one would have

$$
\begin{equation*}
\mathcal{P}\left(f_{\theta}\right)(x)=\underset{z \in \mathbb{R}^{n_{\text {out }}}}{\arg \min }\left\|z-f_{\theta}(x)\right\|_{2} \text { s.t. } A(x) z \leq b(x) . \tag{1}
\end{equation*}
$$

Although the constraints are affine, this optimization does not admit a closed-form solution in general for more than one constraint, making it computationally expensive especially as it needs to be computed for every sample $x$ (during training/inference) and every parameter $\theta$ (during training).

In case of a single constraint $a(x)^{\top} f(x) \leq b(x) \forall x \in \mathcal{X}$ with $a(x) \in \mathbb{R}^{n_{\text {out }}}$ and $b(x) \in \mathbb{R}$, recent work in the control literature-for instance, Kolter and Manek (2019) for learning stable dynamics and Donti et al. (2021a); Min et al. (2023) for learning stabilizing controllers-has adopted the following closed-form solution, which is differentiable almost everywhere:

$$
\begin{align*}
\mathcal{P}\left(f_{\theta}\right)(x) & =\underset{z \in \mathbb{R}^{n_{\text {out }}}}{\arg \min }\left\|z-f_{\theta}(x)\right\|_{2} \text { s.t. } a(x)^{\top} z \leq b(x)  \tag{2}\\
& =f_{\theta}(x)-\frac{a(x)}{\|a(x)\|^{2}} \operatorname{ReLU}\left(a(x)^{\top} f_{\theta}(x)-b(x)\right) \tag{3}
\end{align*}
$$

Example $1 a(x)=[-1 ; x]$ and $b(x)=0$ encode the constraint $(f(x))_{(0)} \geq x(f(x))_{(1)}$ on $f: \mathbb{R} \rightarrow \mathbb{R}^{2}$. Then, a sample $f_{\theta}(1)=[3 ; 5]$ is projected to $\mathcal{P}\left(f_{\theta}\right)(1)=[4 ; 4]$, satisfying the constraint.

Nonetheless, the formulation is limited to enforcing only a single inequality constraint. Moreover, its empirical success in learning the desired functions has not been theoretically
understood. To that end, we propose HardNet-Aff, the first method, to the best of our knowledge, that enables efficient (closed-form) and differentiable enforcement of more than one input-dependent inequality constraint. In addition, we provide a rigorous explanation for its expressivity through universal approximation guarantees. Then, we discuss HardNet-Cvx as a conceptual framework to satisfy general input-dependent convex constraints exploiting differentiable convex optimization solvers.

### 4.1 HardNet-Aff: Imposing Input-Dependent Affine Constraints

Suppose we have multiple input-dependent affine constraints in an aggregated form:

$$
\begin{equation*}
b^{l}(x) \leq A(x) f(x) \leq b^{u}(x) \quad \forall x \in \mathcal{X} \tag{4}
\end{equation*}
$$

where $A(x) \in \mathbb{R}^{n_{\mathrm{c}} \times n_{\text {out }}}$ and $b^{l}(x), b^{u}(x) \in \mathbb{R}^{n_{\mathrm{c}}}$ for $2 n_{\mathrm{c}}$ inequality constraints.
Remark 3 The constraint form in (4) is general and includes equality constraints by setting $b^{l}=b^{u}$. Another approach to enforcing equality constraints is to have the neural network predict only a subset of the outputs, with the remaining components computed to satisfy the equality constraints, as in Beucler et al. (2021) and Donti et al. (2021b). However, that method may fail if the chosen subset is invalid for certain inputs. For example, the constraint $[x x-1] f(x)=1$ on $f: \mathbb{R} \rightarrow \mathbb{R}^{2}$ prevents either component from being a free variable at both $x=0$ and 1 . See Appendix $B$ for incorporating this approach into our method.

Remark 4 One-sided inequality constraints can be represented by setting $b^{l}=-\infty$ or $b^{u}=\infty$. In addition, different types of constraints (inequality and equality) can be combined by specifying $b^{l}$ and $b^{u}$ component-wise.

Assumption 5 For each $x \in \mathcal{X}$, i) there exists $y \in \mathbb{R}^{n_{\text {out }}}$ that satisfies the constraints in (4), and ii) $A(x)$ has full row rank.

The first assumption says that the constraints (4) are feasible, while the second one requires $n_{\mathrm{c}} \leq n_{\text {out }}$. Under the assumptions, we propose HardNet-Aff by developing a novel closed-form projection that enforces the constraints (4) as below:

HardNet-Aff: $\mathcal{P}\left(f_{\theta}\right)(x)=f_{\theta}(x)+A(x)^{+}\left[\operatorname{ReLU}\left(b^{l}(x)-A(x) f_{\theta}(x)\right)-\operatorname{ReLU}\left(A(x) f_{\theta}(x)-b^{u}(x)\right)\right]$,
for all $x \in \mathcal{X}$ where $M^{+}:=M^{\top}\left(M M^{\top}\right)^{-1}$ denotes the pseudoinverse of a matrix $M$. Note that the projection in (5) generalizes the single-constraint case in (3) with $n_{c}=1$ and $b^{l}=-\infty$. However, unlike (3), HardNet-Aff in general does not perform the minimum $\ell^{2}$-norm projection as in (2). Instead, we can characterize its projection through the following optimization problem; see Appendix A. 1 for a proof.

Proposition 6 Under Assumption 5, for any parameterized function $f_{\theta}: \mathcal{X} \rightarrow \mathbb{R}^{n_{\text {out }}}$, HardNet-Aff in (5) satisfies

$$
\begin{align*}
\mathcal{P}\left(f_{\theta}\right)(x)=\underset{z \in \mathbb{R}^{n_{\text {out }}}}{\arg \min } \| z & -f_{\theta}(x) \|_{2} \\
\text { s.t. } & z \in\left\{\underset{y \in \mathbb{R}^{n_{\text {out }}}}{\arg \min }\left\|A(x)\left(y-f_{\theta}(x)\right)\right\|_{2} \text { s.t. } b^{l}(x) \leq A(x) y \leq b^{u}(x)\right\} \tag{6}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/f547083e-7be0-42e8-9246-1bc5914287ea-09.jpg?height=480&width=1148&top_left_y=305&top_left_x=487)
Figure 2: Illustration of input-dependent constraints and projections performed by HardNet. A target function $f: \mathbb{R} \rightarrow \mathbb{R}^{2}$ satisfies hard constraints $f(x) \in \mathcal{C}(x)$ for each $x \in \mathbb{R}$. The feasible set $\mathcal{C}(x)$ is visualized as the gray area for two sample inputs $x_{1}$ and $x_{2}$. While the function $f_{\theta}$ closely approximates $f$, it violates the constraints. HardNet-Aff projects the violated output onto the feasible set in parallel to the boundaries of the satisfied constraints. In contrast, the minimum $\ell^{2}$-norm optimization in (1) projects the output orthogonally to the closest boundary.

Thus, HardNet-Aff satisfies the constraint (4) while minimally altering the output from the plain output $f_{\theta}(x)$ in the sense of (6). We note that the inner optimization in (6) has infinitely many solutions when $A(x)$ has a non-zero null space (i.e., $n_{c}<n_{\text {out }}$ ). In such cases, HardNet-Aff chooses the solution that is closest (in Euclidean distance) to the plain output $f_{\theta}(x)$. On the other hand, for a square matrix $A(x)$, the inner optimization has a unique solution while the square of its objective function is the Bregman divergence generated from the function $\psi_{x}(y)=\|A(x) y\|_{2}^{2}$.

In addition to the optimization formulation, HardNet-Aff can also be understood geometrically through the following property; see Appendix A. 2 for a proof.

Proposition 7 (Parallel Projection) Under Assumption 5, for any parameterized function $f_{\theta}: \mathcal{X} \rightarrow \mathbb{R}^{n_{\text {out }}}$, for each $i$-th row $a_{i} \in \mathbb{R}^{n_{\mathrm{c}}}$ of $A(x)$, HardNet-Aff in (5) satisfies

$$
a_{i}^{\top} \mathcal{P}\left(f_{\theta}\right)(x)=\left\{\begin{array}{l}
b_{(i)}^{l}(x) \text { if } a_{i}^{\top} f_{\theta}(x)<b_{(i)}^{l}(x)  \tag{7}\\
b_{(i)}^{u}(x) \text { if } a_{i}^{\top} f_{\theta}(x)>b_{(i)}^{u}(x) \quad \text { for all } x \in \mathcal{X} . \\
a_{i}^{\top} f_{\theta}(x) \quad \text { o.w. }
\end{array}\right.
$$

If $f_{\theta}(x)$ violates any constraints, it is projected onto the boundary of the feasible set $\left(a_{i}^{\top} \mathcal{P}\left(f_{\theta}\right)(x)=b_{(i)}^{l}(x)\right.$ or $\left.b_{(i)}^{u}(x)\right)$. Notably, the projection alters the output in a direction parallel to the boundary of each satisfied constraint's feasible set $\left(a_{i}^{\top} \mathcal{P}\left(f_{\theta}\right)(x)=a_{i}^{\top} f_{\theta}(x)\right)$. This parallel projection is illustrated in Fig. 2.

While HardNet-Aff guarantees the satisfaction of the hard constraints (4), it should not lose the neural networks' expressivity for practical deployment. To that end, we rigorously show that HardNet-Aff preserves the neural networks' expressivity by the following theorem; see Appendix A. 3 for a proof.

Theorem 8 Consider input-dependent constraints (4) that satisfy assumption 5. Suppose $\mathcal{X} \subset \mathbb{R}^{n_{\mathrm{in}}}$ is compact, and $A(x)$ is continuous over $\mathcal{X}$. Then, for any function classes $\mathcal{F}_{\mathrm{NN}}, \mathcal{F} \subset \mathcal{C}\left(\mathcal{X}, \mathbb{R}^{n_{\text {out }}}\right)$ (or $\mathcal{F}_{\mathrm{NN}}, \mathcal{F} \subset L^{p}\left(\mathcal{X}, \mathbb{R}^{n_{\text {out }}}\right)$ for any $p \in[1, \infty)$ ) and the projection $\mathcal{P}$ of HardNet-Aff in (5), if $\mathcal{F}_{\mathrm{NN}}$ universally approximates $\mathcal{F}$, $\mathcal{F}_{\text {HardNet-Aff }}:=\left\{\mathcal{P}\left(f_{\mathrm{NN}}\right) \mid f_{\mathrm{NN}} \in \mathcal{F}_{\mathrm{NN}}\right\}$ universally approximates $\mathcal{F}_{\text {target }}:=\left\{f_{t} \in \mathcal{F} \mid f_{t}\right.$ satisfies (4) $\}$.

Considering $\mathcal{F}_{\mathrm{NN}}$ as the function class of plain neural networks, this theorem shows that HardNet-Aff preserves their expressivity over $\mathcal{F}$ under the constraints (4). The main idea behind this theorem is that for HardNet-Aff in $(5),\left\|f-\mathcal{P}\left(f_{\theta}\right)\right\|$ could be bounded in terms of $\left\|f-f_{\theta}\right\|$. By selecting $f_{\theta}$ such that $\left\|f-f_{\theta}\right\|$ is arbitrarily small, we can make $\mathcal{P}\left(f_{\theta}\right)$ approach the target function $f$ as closely as desired. The existence of such an $f_{\theta}$ can be guaranteed by existing universal approximation theorems on plain neural networks. For instance, if we utilize Theorem 2 in Theorem 8, we can obtain the following universal approximation theorem for HardNet-Aff.

Corollary 9 Suppose the assumptions of Theorem 8 hold. Then, for any $p \in[1, \infty)$, HardNet-Aff with w-width ReLU neural networks defined in (5) universally approximates $\mathcal{F}_{\text {target }}:=\left\{f_{t} \in L^{p}\left(\mathcal{X}, \mathbb{R}^{n_{\text {out }}}\right) \mid f_{t}\right.$ satisfies (4) $\}$ if $w \geq \max \left\{n_{\text {in }}+1, n_{\text {out }}\right\}$.

### 4.2 HardNet-Cvx: Imposing Input-Dependent Convex Constraints

Going beyond affine constraints, we discuss HardNet-Cvx as a conceptual framework for enforcing general input-dependent convex constraints $f(x) \in \mathcal{C}(x) \forall x \in \mathcal{X}$ where $\mathcal{C}(x) \subset \mathbb{R}^{n_{\text {out }}}$ is a convex set. Unlike the affine case, the closed-form projection from the single-constraint case in (3) does not extend to general convex constraints. Thus, we present HardNet-Cvx by generalizing the optimization-based projection in (2) as below:

$$
\begin{equation*}
\text { HardNet-Cvx: } \mathcal{P}\left(f_{\theta}\right)(x)=\underset{z \in \mathbb{R}^{n_{\mathrm{out}}}}{\arg \min }\left\|z-f_{\theta}(x)\right\|_{2} \text { s.t. } z \in \mathcal{C}(x) \tag{8}
\end{equation*}
$$

for all $x \in \mathcal{X}$. While no general closed-form solution for this optimization problem exists, we can employ differentiable convex optimization solvers for an implementation of HardNet-Cvx such as Amos and Kolter (2017) for affine constraints (when HardNet-Aff cannot be applied) and Agrawal et al. (2019) for more general convex constraints. This idea was briefly mentioned by Donti et al. (2021b) and used as a baseline (for input-independent constraints) by Tordesillas et al. (2023). We present HardNet-Cvx as a general framework, independent of specific implementation methods, to complement HardNet-Aff and provide a unified solution for various constraint types.

As in Section 4.1, we demonstrate that HardNet-Cvx preserves the expressive power of neural networks by proving the following universal approximation theorem; see Appendix A. 4 for a proof.

Theorem 10 Consider input-dependent sets $\mathcal{C}(x) \subset \mathbb{R}^{n_{\text {out }}}$ that are convex for all $x \in \mathcal{X} \subset \mathbb{R}^{n_{\mathrm{in}}}$. Then, for any $p \in[1, \infty)$, HardNet-Cvx with w-width ReLU neural networks defined in (8) universally approximates $\mathcal{F}_{\text {target }}:=\left\{f_{t} \in L^{p}\left(\mathcal{X}, \mathbb{R}^{n_{\text {out }}}\right) \mid f_{t}(x) \in \mathcal{C}(x) \forall x \in \mathcal{X}\right\}$ if $w \geq \max \left\{n_{\text {in }}+1, n_{\text {out }}\right\}$.

![](https://cdn.mathpix.com/cropped/f547083e-7be0-42e8-9246-1bc5914287ea-11.jpg?height=506&width=1486&top_left_y=313&top_left_x=317)
Figure 3: Learned functions at the initial (left) and final (right) epochs with the piecewise constraints. The models are trained on the samples indicated with circles, with their MSE from the true function shown in parentheses. HardNet-Aff adheres to the constraints from the start of the training and generalizes better than the baselines as it enforces constraints even in the out-of-distribution (OOD) regime. On the other hand, the baselines violate the constraints throughout the training.

## 5 Experiments

In this section, we demonstrate the versatility and effectiveness of HardNet-Aff over three constrained scenarios: learning with piecewise constraints, learning optimization solvers with guaranteed feasibility, and optimizing control policies in safety-critical systems.

As evaluation metrics, we measure the violation of constraints in addition to the application-specific performance metrics. For a test sample $x \in \mathcal{X}$ and $n_{\text {ineq }}$ inequality constraints $g_{x}(f(x)) \leq 0 \in \mathbb{R}^{n_{\text {ineq }}}$, their violation is measured with the maximum ( $\leq \max$ ) and mean ( $\leq$ mean) of $\operatorname{ReLU}\left(g_{x}(f(x))\right)$ and the number of violated constraints ( $\leq \#$ ). Similar quantities of $\left|h_{x}(f(x))\right|$ are measured for $n_{\text {eq }}$ equality constraints $h_{x}(f(x))=0 \in \mathbb{R}^{n_{\text {eq }}}$. Then, they are averaged over all test samples. The inference time ( $T_{\text {test }}$ ) for the test set and the training time ( $T_{\text {train }}$ ) are also compared.

We compare HardNet with the following baselines: (i) NN: Plain neural networks, (ii) Soft: Soft-constrained neural networks, where constraint violations are penalized by adding regularization terms $\lambda\left\|\operatorname{ReLU}\left(g_{x}\left(f\left(x_{s}\right)\right)\right)\right\|_{2}^{2}+\lambda\left\|h_{x}\left(f\left(x_{s}\right)\right)\right\|_{2}^{2}$ to the loss function for each sample $x_{s} \in \mathcal{X}$, (iii) DC3 (Donti et al., 2021b): Similarly to HardNet-Aff, DC3 approximates part of the target function with a neural network. It first augments the output to satisfy equality constraints, then corrects it using gradient descent to minimize inequality violations. DC3 backpropagates through this iterative correction procedure to train the model. All methods use 3 -layer fully connected neural networks with 200 neurons per hidden layer and ReLU activation. The results are produced with Intel Xeon Gold 6248 and NVidia Volta V100.

Table 2: Results for learning with the piecewise constraints. HardNet-Aff generalizes better than the baselines with the smallest MSE from the true function without any constraint violation. The max and mean of constraint violations are computed over 401 test samples. Better (resp. worse) values are colored greener (resp. redder). Standard deviations over 5 runs are shown in parentheses.
|  | MSE | max violation | mean violation | $T_{\text {test }}(\mathrm{ms})$ | $T_{\text {train }}(\mathrm{s})$ |
| :--- | :---: | :---: | :---: | :---: | :---: |
| NN | $1.85(0.19)$ | $3.16(0.29)$ | $0.60(0.04)$ | $0.14(0.00)$ | $2.24(0.09)$ |
| Soft | $2.36(0.22)$ | $3.70(0.25)$ | $0.69(0.03)$ | $0.15(0.02)$ | $3.86(0.03)$ |
| DC3 | $1.15(0.10)$ | $2.31(0.23)$ | $0.43(0.02)$ | $6.02(0.07)$ | $15.67(0.82)$ |
| HardNet-Aff | $0.16(0.01)$ | $0.00(0.00)$ | $0.00(0.00)$ | $0.88(0.05)$ | $5.62(0.06)$ |


Table 3: Results for learning solvers of nonconvex optimization problems with 100 variables, 50 equality constraints, and 50 inequality constraints. HardNet-Aff attain feasible solutions with the smallest suboptimality gap among the feasible methods. The max, mean, and the number of violations are computed out of the 50 constraints. Better (resp. worse) values are colored greener (resp. redder). Standard deviations over 5 runs are shown in parentheses.
|  | Obj. val | $\not \leq \max /$ mean $/ \#$ | $\neq \max /$ mean $/ \#$ | $T_{\text {test }}(\mathrm{ms})$ | $T_{\text {train }}(\mathrm{s})$ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Optimizer | -14.28 (0.00) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 1182.0 (3.49) | - |
| NN | -27.43 (0.00) | 12.1/1.1/12.0 (0.0/0.0/0.0) | 15.1/6.4/50.0 (0.0/0.0/0.0) | 0.32 (0.06) | 78.10 (2.36) |
| Soft | -13.13 (0.01) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 0.4/0.1/50.0 (0.0/0.0/0.0) | 0.37 (0.07) | 77.58 (0.97) |
| DC3 | -12.57 (0.04) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 9.61 (0.47) | 3606.74 (2.45) |
| HardNet-Aff | -14.10 (0.01) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 0.0/0.0/0.0 (0.0/0.0/0.0) | 6.69 (0.06) | 1343.80 (1.39) |


### 5.1 Learning with Piecewise Constraints

In this experiment, we demonstrate the efficacy of HardNet-Aff and the expressive power of input-dependent constraints on a problem involving learning a function $f:[-2,2] \rightarrow \mathbb{R}$ with piecewise constraints shown in Fig. 3. The function outputs are required to avoid specific regions defined over separate subsets of the domain $[-2,2]$. An arbitrary number of such piecewise constraints can be captured by a single input-dependent affine constraint. The models are trained on 50 labeled data points randomly sampled from [ $-1.2,1.2$ ]; see Appendix D. 1 for details. Additionally, we assess HardNet-Aff with more complex constraints in which each regime is governed by both upper and lower bounds (or, in the degenerate case, an equality) in Appendix D.2.

As shown in Fig. 3 and Table 2, HardNet-Aff consistently satisfies the hard constraints throughout training and achieves better generalization than the baselines, which violate these constraints. Especially at the boundaries $x=-1$ and $x=1$ in the initial epoch results, the jumps in DC3's output value, caused by DC3's correction process, insufficiently reduce the constraint violations. The performance of its iterative correction process heavily depends on the number of gradient descent steps and the step size. DC3 requires careful hyperparameter tuning unlike HardNet-Aff.

Table 4: Results for optimizing safe control policies. HardNet-Aff generates trajectories without constraint violation and has the smallest costs among the methods with zero violation. The max and mean constraint violations are computed for the violations accumulated throughout the trajectories. Better values are colored greener. Standard deviations over 5 runs are shown in parentheses.
|  | Cost | max violation | mean violation | $T_{\text {test }}(\mathrm{ms})$ | $T_{\text {train }}(\min )$ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| CBF-QP | 948.32 (0.00) | 0.00 (0.00) | 0.00 (0.00) | 579.29 (4.60) | - |
| NN | 421.92 (0.15) | 157.62 (0.90) | 118.92 (0.55) | 0.23 (0.01) | 256.34 (4.01) |
| Soft | 480.10 (0.54) | 6.92 (0.05) | 3.95 (0.10) | 0.22 (0.00) | 255.07 (0.77) |
| DC3 | 480.21 (0.71) | 6.86 (0.13) | 3.88 (0.12) | 15.71 (0.29) | 637.69 (6.71) |
| HardNet-Aff | 518.85 (8.71) | 0.00 (0.00) | 0.00 (0.00) | 2.71 (0.06) | 370.05 (3.42) |


### 5.2 Learning Optimization Solvers with Guaranteed Feasibility

We consider learning optimization solvers with the following nonconvex optimization as in (Donti et al., 2021b):

$$
f(x)=\underset{y}{\arg \min } \frac{1}{2} y^{\top} Q y+p^{\top} \sin y \text { s.t. } A y \leq b, C y=x \text {, }
$$

where $Q \in \mathbb{R}^{n_{\text {out }} \times n_{\text {out }}} \succeq 0, p \in \mathbb{R}^{n_{\text {out }}}, A \in \mathbb{R}^{n_{\text {ineq }} \times n_{\text {out }}}, b \in \mathbb{R}^{n_{\text {ineq }}}, C \in \mathbb{R}^{n_{\text {eq }} \times n_{\text {out }}}$ are constants and $\sin$ is the element-wise sine function. The target function $f$ outputs the solution of each optimization problem instance determined by the input $x \in[-1,1]^{n_{\text {eq }}}$. The main benefit of learning this nonconvex optimization solver with neural networks is their faster inference time than optimizers based on iterative methods. To ensure that the learned neural networks provide feasible solutions, the constraints of the optimization problems are set as hard constraints.

In this experiment, we guarantee that the given constraints are feasible for all $x \in[-1,1]^{n_{\mathrm{eq}}}$ by computing a proper $b$ for randomly generated $A, C$ as described in (Donti et al., 2021b). Then the models are trained on 10000 unlabeled data points uniformly sampled from $[-1,1]^{n_{\text {eq }}}$. For this unsupervised learning task, the loss function for each sample $x_{s}$ is set as $\frac{1}{2} f_{\theta}\left(x_{s}\right)^{\top} Q f_{\theta}\left(x_{s}\right)+p^{\top} \sin f_{\theta}\left(x_{s}\right)$. To reproduce similar results as in (Donti et al., 2021b), the models are equipped with additional batch normalization and dropout layers in this experiment. As shown in Table 3, HardNet-Aff consistently finds feasible solutions with a small suboptimality gap from the optimizer (IPOPT) with a much shorter inference time.

### 5.3 Optimizing Control Policies in Safety-Critical Systems

In this experiment, we apply HardNet-Aff to enforce safety constraints in control systems. Consider a control-affine system with its known dynamics $f$ and $g: \dot{x}(t)=f(x(t))+g(x(t)) u(t)$, where $x(t) \in \mathbb{R}^{n_{\text {in }}}$ is the system state, and $u(t) \in \mathbb{R}^{n_{\text {out }}}$ is the control input at time $t$. For safety reasons (e.g., avoiding obstacles), the system requires $x(t) \in \mathcal{X}_{\text {safe }} \subset \mathbb{R}^{n_{\text {in }}}$ for all $t$. We translate this safety condition into a state-dependent affine constraint on the control input using a control barrier function (CBF) $h: \mathbb{R}^{n_{\mathrm{in}}} \rightarrow \mathbb{R}$ (Ames et al., 2019). Suppose its super-level set $\left\{x \in \mathbb{R}^{n_{\text {in }}} \mid h(x) \geq 0\right\} \subset \mathcal{X}_{\text {safe }}$ and $h(x(0)) \geq 0$. Then, we can ensure

![](https://cdn.mathpix.com/cropped/f547083e-7be0-42e8-9246-1bc5914287ea-14.jpg?height=632&width=743&top_left_y=320&top_left_x=691)
Figure 4: Simulated trajectories from a random initial state, with costs shown in parentheses. HardNet-Aff avoids the obstacles while obtaining a low cost value. Even though the soft-constrained method and DC3 appear to avoid obstacles and achieve smaller costs than the other collision-free trajectories, they violate the safety constraints (which are more conservative than hitting the obstacles).

$h(x(t)) \geq 0 \forall t \geq 0$ by guaranteeing

$$
\begin{equation*}
\dot{h}(x)=\nabla h(x)^{\top}(f(x)+g(x) \pi(x)) \geq-\alpha h(x) \tag{9}
\end{equation*}
$$

at each $x(t)$ for a state-feedback control policy $\pi: \mathbb{R}^{n_{\text {in }}} \rightarrow \mathbb{R}^{n_{\text {out }}}$ with some $\alpha>0$. Enforcing (9) for multiple CBFs ensures the trajectory remains within the intersection of the corresponding safe sets.

We consider controlling a unicycle system to minimize the cost over trajectories while avoiding collisions with two elliptical obstacles, each presented with a CBF (see Appendix D. 3 for details). Then, we can formulate the problem as an optimization problem with its objective function being the expected cost over the trajectory $x(t)$ generated by a parameterized state feedback policy $\pi_{\theta}: \mathbb{R}^{n_{\text {in }}} \rightarrow \mathbb{R}^{n_{\text {out }}}$ from random initial point $x(0) \sim \mathcal{D}$ :

$$
\begin{equation*}
\underset{\theta}{\arg \min } \underset{x(0) \sim \mathcal{D}}{\mathbb{E}} \int_{t=0}^{T}\left[x^{\top} Q x+\pi_{\theta}(x)^{\top} R \pi_{\theta}(x)\right] d t \text { s.t. (9) holds for all } \mathrm{CBFs} \forall t \in[0, T] \forall i \tag{10}
\end{equation*}
$$

where $Q$ is the state cost matrix and $R$ is the control cost matrix.
Given a nominal controller $\pi_{\text {nom }}: \mathbb{R}^{n_{\text {in }}} \rightarrow \mathbb{R}^{n_{\text {out }}}$ designed without considering obstacles, a conventional approach to find a safe controller is to solve the following quadratic program at each $x(t)$ :

$$
\mathrm{CBF}-\mathrm{QP}: \pi_{\mathrm{CBF}-\mathrm{QP}}(x)=\underset{u}{\arg \min }\left\|u-\pi_{\mathrm{nom}}(x)\right\|_{2} \text { s.t. (9) holds for all CBFs. }
$$

The downside of this method is that the controller cannot optimize a cost/reward over trajectories, as it only stays close to the nominal controller. Instead, we can do so by training

## HardNet: Hard-Constrained Neural Networks

neural network policies $\pi_{\theta}(x):=\pi_{\text {nom }}(x)+f_{\theta}(x)$ with neural networks $f_{\theta}$. For computation, we approximate (10) by minimizing the costs of rolled-out trajectories from randomly sampled initial states. As shown in Fig. 4 and Table 4, HardNet-Aff consistently guarantees safe trajectories with low costs.

## 6 Conclusion

In this paper, we presented HardNet, a practical framework for constructing neural networks that inherently satisfy input-output constraints. We proved that imposing these hard constraints does not limit the expressive power of these neural networks by providing universal approximation guarantees. We demonstrated the utility and versatility of our method across several applications, such as learning with piecewise constraints, learning optimization solvers with guaranteed feasibility, and optimizing control policies in safetycritical systems. Using HardNet in other application domains that benefit from incorporating domain-specific knowledge is a promising direction for future work. One such example is in Tang et al. (2024). Also, we aim to explore developing methods for performing fast projections for problems with more general constraints. Lastly, extending our approach to support other forms of inductive biases, such as equivariances and invariances, would potentially be of great interest.

## Acknowledgments

The authors thank Anoopkumar Sonar for his help during the early development of this manuscript. The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing computing resources that have contributed to the results reported within this paper. This work was supported in part by MathWorks, the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, and the MIT-Google Program for Computing Innovation.

## References

A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. Advances in neural information processing systems, 32, 2019.
K. Ahmed, S. Teso, K.-W. Chang, G. Van den Broeck, and A. Vergari. Semantic probabilistic layers for neuro-symbolic learning. Advances in Neural Information Processing Systems, 35:29944-29959, 2022.
A. Albarghouthi et al. Introduction to neural network verification. Foundations and Trends Â® in Programming Languages, 7(1-2):1-157, 2021.
A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath, and P. Tabuada. Control barrier functions: Theory and applications. In 2019 18th European control conference (ECC), pages 3420-3431. IEEE, 2019.
B. Amos and J. Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136-145. PMLR, 2017.


[^0]:    1. The code is available at https://github.com/azizanlab/hardnet
